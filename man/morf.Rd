% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/morf.R
\name{morf}
\alias{morf}
\title{Modified Ordered Random Forest for Estimating the Ordered Choice Model}
\usage{
morf(
  x = NULL,
  y = NULL,
  n.trees = 1000,
  mtry = NULL,
  min.node.size = 5,
  max.depth = 0,
  replace = FALSE,
  sample.fraction = ifelse(replace, 1, 0.632),
  case.weights = NULL,
  split.select.weights = NULL,
  always.split.variables = NULL,
  keep.inbag = FALSE,
  inbag = NULL,
  holdout = FALSE,
  oob.error = TRUE,
  n.threads = NULL,
  save.memory = FALSE,
  write.forest = TRUE,
  verbose = TRUE,
  seed = NULL
)
}
\arguments{
\item{x}{Covariate matrix (no intercept).}

\item{y}{Outcome vector.}

\item{n.trees}{Number of trees.}

\item{mtry}{Number of covariates to possibly split at in each node. Default is the (rounded down) square root of the number of covariates. Alternatively, one can pass a single-argument function returning an integer, where the argument is the number of covariates.}

\item{min.node.size}{Minimal node size.}

\item{max.depth}{Maximal tree depth. A value of 0 (the default) corresponds to unlimited depth, 1 to "stumps" (one split per tree).}

\item{replace}{If \code{TRUE}, grow trees on bootstrap subsamples. Otherwise, trees are grown on random subsamples drawn without replacement.}

\item{sample.fraction}{Fraction of observations to sample. Default is 1 for bootstrap sampling and 0.632 for sampling without replacement.}

\item{case.weights}{Weights for sampling of training observations. Observations with larger weights will be drawn with higher probability in the subsamples for the trees.}

\item{split.select.weights}{Numeric vector with weights between 0 and 1, used to calculate the probability to select variables for splitting. Alternatively, a list of size \code{n.trees}, containing split select weight vectors for each tree can be used.}

\item{always.split.variables}{Character vector with variable names to be always selected in addition to the \code{mtry} variables tried for splitting.}

\item{keep.inbag}{Save how often observations are in-bag in each tree.}

\item{inbag}{Manually set observations per tree. List of size \code{n.trees}, containing in-bag counts for each observation. Can be used for stratified sampling.}

\item{holdout}{Hold-out mode. Hold-out all samples with zero \code{case.weights} and use these for variable importance and prediction error.}

\item{oob.error}{Compute out-of-bag prediction error. Set to \code{FALSE} to save computation time.}

\item{n.threads}{Number of threads. Default is number of CPUs available.}

\item{save.memory}{Use memory saving (but slower) splitting mode. Warning: This option slows down the tree growing, use only if you encounter memory problems.}

\item{write.forest}{Save \code{morf.forest} object, required for prediction. Set to \code{FALSE} to reduce memory usage if no prediction intended.}

\item{verbose}{Show computation status and estimated runtime.}

\item{seed}{Random seed. Default is \code{NULL}, which generates the seed from \code{R}. Set to \code{0} to ignore the \code{R} seed.}
}
\value{
Object of class \code{morf} with elements:
  \item{\code{forest.1}}{\code{morf.forest} object of the first class.}
  \item{\code{forest.2}}{\code{morf.forest} object of the second class.} 
  \item{\code{...}}{}
  \item{\code{forest.M}}{\code{morf.forest} object of the last class.}
  \item{\code{predictions}}{Matrix of predicted conditional class probabilities, based on out-of-bag samples. Requires \code{oob.error = TRUE} (the default).}
  \item{\code{mean.squared.error}}{Mean squared error of the model, based on out-of-bag predictions. Requires \code{oob.error = TRUE} (the default).}
  \item{\code{mean.ranked.score}}{Mean squared error of the model, based on out-of-bag predictions. Requires \code{oob.error = TRUE} (the default).}
  \item{\code{overall.importance}}{Measure of overall variable importance, computed as the mean of the importance of each variable across classes. Relative importance is provided.}
  \item{\code{n.classes}}{Number of classes.}
  \item{\code{n.samples}}{Number of observations.}
  \item{\code{n.covariates}}{Number of covariates.}
  \item{\code{n.trees}}{Number of trees of each forest.}
  \item{\code{mtry}}{Number of covariates considered for splitting at each step.}
  \item{\code{min.node.size}}{Minimum node size.}
  \item{\code{replace}}{Whether the subsamples to grow trees are drawn with replacement (bootstrap).}
  \item{\code{data}}{Training sample.}
  \item{\code{call}}{The system call.}
}
\description{
Non-parametric estimation of the ordered choice model using random forests.
}
\details{
\subsection{Splitting Criterion}{
\code{morf} fits \code{M} random forests, where \code{M} is the number of classes of \code{y}. 
Each forest computes the conditional probabilities of the \code{m}-th class:

\deqn{p_m (x) = P ( Y_i = m \, | \, X_i = x), \,\,\,  m = 1, ..., M}

To estimate this quantity, \code{morf} exploits the following:

\deqn{p_m (x) = E [\, 1 (Y_i \leq m) \, | \, X_i = x] - E[\, 1 ( Y_i \leq m - 1 ) \, | \, X_i = x]  
                = \mu_m (x) - \mu_{m-1} (x)}

with \code{1(.)} an indicator of the truth of its argument. It would be tempting to estimate the 
conditional probabilities as:

\deqn{\hat{p}_m (x) = \hat{\mu}_m (x) - \hat{\mu}_{m-1} (x)}

where the expectations can be estimated via separate regression forests. However, this strategy 
ignores potential correlation in the estimation errors of the two surfaces, as shown by the following expansion:

\deqn{MSE[\hat{p}_m (x)] = MSE[\hat{\mu}_m (x)] + MSE [\hat{\mu}_{m-1} (x)] - 2 MCE[\hat{\mu}_m (x), \hat{\mu}_{m-1} (x)]}

where the last term is the mean correlation error of the estimation. The splitting criterion used by \code{morf} 
seeks to greedily minimize the above expression, thereby accounting for the correlation term. Therefore, each split
is chosen to minimize:

\deqn{Var( 1 (Y_i \leq m)) + Var( 1 (Y_i \leq m - 1)) - 2 * Cor(1 (Y_i \leq m), 1 (Y_i \leq m - 1))}
}

\subsection{Predictions}{
Predictions in the \code{l}-th leaf are computed as:

\deqn{\frac{1}{\{i: x_i \in L_l\}} \sum_{\{i: x_i \in L_l\}} 1 (Y_i \leq m) - \frac{1}{\{i: x_i \in L_l\}} \sum_{\{i: x_i \in L_l\}} 1 (Y_i \leq m - 1)}

Notice that a normalization step may be needed to ensure that the estimated probabilities sum up to one across classes.
}

\subsection{Variable Importance}{
For each covariate, an overall variable importance measure is provided. The \code{m}-th forest computes the 
importance of the j-th covariate for the \code{m}-th class by recording the improvement in the splitting criterion 
at each split placed on such covariate. Summing over all such splits of the trees in the \code{m}-th 
forest gives the j-th covariate's importance for the \code{m}-th class. The overall variable importance measure of this 
covariate is then defined as the mean of its importances in each class.
}
}
\references{
\itemize{
  \item Lechner, M., & Okasa, G. (2019). Random forest estimation of the ordered choice model. arXiv preprint arXiv:1907.02436. \doi{10.48550/arXiv.1907.02436}.
  \item Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. J Stat Softw 77:1-17. \doi{10.18637/jss.v077.i01}.
}
}
\seealso{
\code{\link{predict.morf}}, \code{\link{marginal_effects}}, \code{\link{mean_squared_error}},
\code{\link{mean_ranked_score}}.
}
\author{
Riccardo Di Francesco
}
