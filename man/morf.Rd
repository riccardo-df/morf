% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/morf.R
\name{morf}
\alias{morf}
\title{Modified Ordered Random Forest for Estimating the Ordered Choice Model}
\usage{
morf(
  x = NULL,
  y = NULL,
  n.trees = 1000,
  mtry = NULL,
  min.node.size = 5,
  max.depth = 0,
  replace = FALSE,
  sample.fraction = ifelse(replace, 1, 0.632),
  case.weights = NULL,
  honesty = TRUE,
  honesty.fraction = 0.5,
  inference = FALSE,
  split.select.weights = NULL,
  always.split.variables = NULL,
  keep.inbag = FALSE,
  inbag = NULL,
  holdout = FALSE,
  oob.error = FALSE,
  n.threads = NULL,
  save.memory = FALSE,
  write.forest = TRUE,
  verbose = TRUE,
  seed = NULL
)
}
\arguments{
\item{x}{Covariate matrix (no intercept).}

\item{y}{Outcome vector.}

\item{n.trees}{Number of trees.}

\item{mtry}{Number of covariates to possibly split at in each node. Default is the (rounded down) square root of the number of covariates. Alternatively, one can pass a single-argument function returning an integer, where the argument is the number of covariates.}

\item{min.node.size}{Minimal node size.}

\item{max.depth}{Maximal tree depth. A value of 0 corresponds to unlimited depth, 1 to "stumps" (one split per tree).}

\item{replace}{If \code{TRUE}, grow trees on bootstrap subsamples. Otherwise, trees are grown on random subsamples drawn without replacement.}

\item{sample.fraction}{Fraction of observations to sample.}

\item{case.weights}{Weights for sampling of training observations. Observations with larger weights will be drawn with higher probability in the subsamples for the trees.}

\item{honesty}{Whether to grow honest forests.}

\item{honesty.fraction}{Fraction of honest sample. Ignored if \code{honesty = FALSE}.}

\item{inference}{Whether to conduct weight-based inference. It considerably slows down the program.}

\item{split.select.weights}{Numeric vector with weights between 0 and 1, used to calculate the probability to select variables for splitting. Alternatively, one can use a list of size \code{n.trees} containing \code{split.select.weights} vectors, one for each tree.}

\item{always.split.variables}{Character vector with variable names to be always selected in addition to the \code{mtry} variables tried for splitting.}

\item{keep.inbag}{Save how often observations are in-bag in each tree.}

\item{inbag}{Manually set observations per tree. List of size \code{n.trees}, containing in-bag counts for each observation. Can be used for stratified sampling.}

\item{holdout}{Hold-out mode. Hold-out all samples with zero \code{case.weights} and use these for variable importance and prediction error.}

\item{oob.error}{Compute out-of-bag prediction error. Set to \code{FALSE} to save computation time.}

\item{n.threads}{Number of threads. Default is number of CPUs available.}

\item{save.memory}{Use memory saving splitting mode. Warning: This option slows down the tree growing, use only if you encounter memory problems.}

\item{write.forest}{Save \code{morf.forest} object, required for prediction. Set to \code{FALSE} to reduce memory usage if no prediction intended.}

\item{verbose}{Show computation status and estimated runtime.}

\item{seed}{Random seed. Default is \code{NULL}, which generates the seed from \code{R}. Set to \code{0} to ignore the \code{R} seed.}
}
\value{
Object of class \code{morf} with elements:
  \item{\code{forest.1}}{\code{morf.forest} object of the first class.}
  \item{\code{forest.2}}{\code{morf.forest} object of the second class.} 
  \item{\code{...}}{}
  \item{\code{forest.M}}{\code{morf.forest} object of the last class.}
  \item{\code{predictions}}{Matrix of predicted conditional class probabilities. If \code{honesty = TRUE}, these are honest predictions. Otherwise, they are based on out-of-bag samples if \code{oob.error = TRUE}, or an empty list if \code{oob.error = FALSE}.}
  \item{\code{mean.squared.error}}{Mean squared error of the model, based on honest predictions if \code{honesty = TRUE} and on out-of-bag predictions otherwise (the latter requires \code{oob.error = TRUE}).}
  \item{\code{mean.ranked.score}}{Mean ranked probability score of the model, based on honest predictions if \code{honesty = TRUE} and on out-of-bag predictions otherwise (the latter requires \code{oob.error = TRUE}).}
  \item{\code{overall.importance}}{Measure of overall variable importance, computed as the mean of the importance of each variable across classes. Relative importance is provided.}
  \item{\code{n.classes}}{Number of classes.}
  \item{\code{n.samples}}{Number of observations.}
  \item{\code{n.covariates}}{Number of covariates.}
  \item{\code{n.trees}}{Number of trees of each forest.}
  \item{\code{mtry}}{Number of covariates considered for splitting at each step.}
  \item{\code{min.node.size}}{Minimum node size.}
  \item{\code{replace}}{Whether the subsamples to grow trees are drawn with replacement (bootstrap).}
  \item{\code{honesty}}{Whether the forest is honest.}
  \item{\code{honesty.fraction}}{Fraction of units allocated to honest sample.}
  \item{\code{full_data}}{Whole sample.}
  \item{\code{honest_data}}{Honest sample.}
  \item{\code{call}}{The system call.}
}
\description{
Non-parametric estimation of the ordered choice model using random forests.
}
\details{
\subsection{Splitting Criterion}{
\code{morf} fits \code{M} random forests, where \code{M} is the number of classes of \code{y}. 
Each forest computes the conditional probabilities of the \code{m}-th class:

\deqn{p_m (x) = P ( Y_i = m \, | \, X_i = x), \,\,\,  m = 1, ..., M}

To estimate this quantity, \code{morf} exploits the following:

\deqn{p_m (x) = E [\, 1 (Y_i \leq m) \, | \, X_i = x] - E[\, 1 ( Y_i \leq m - 1 ) \, | \, X_i = x]  
                = \mu_m (x) - \mu_{m-1} (x)}

with \code{1(.)} an indicator of the truth of its argument. A straightforward estimator consists of
estimating the two conditional expectations separately and taking the difference:

\deqn{\hat{p}_m (x) = \hat{\mu}_m (x) - \hat{\mu}_{m-1} (x)}

However, this strategy ignores potential correlation in the estimation errors of the two surfaces. An alternative
approach is to minimize the mean squared error of the particular estimation problem:

\deqn{MSE[\check{p}_m (x)] = MSE[\hat{\mu}_m (x)] + MSE [\hat{\mu}_{m-1} (x)] - 2 MCE[\hat{\mu}_m (x), \hat{\mu}_{m-1} (x)]}

where the last term is the mean correlation error of the estimation. \code{morf} ties the estimators of the two
conditional expectations together trying to make the correlation term positive. For this purpose, trees are build 
by greedily minimizing the following expression:

\deqn{Var( 1 (Y_i \leq m)) + Var( 1 (Y_i \leq m - 1)) - 2 * Cor(1 (Y_i \leq m), 1 (Y_i \leq m - 1))}
}

\subsection{Predictions}{
Predictions in the \code{l}-th leaf are computed as:

\deqn{\frac{1}{\{i: x_i \in L_l\}} \sum_{\{i: x_i \in L_l\}} 1 (Y_i \leq m) - \frac{1}{\{i: x_i \in L_l\}} \sum_{\{i: x_i \in L_l\}} 1 (Y_i \leq m - 1)}

Notice that a normalization step may be needed to ensure that the estimated probabilities sum up to one across classes.
}

\subsection{Variable Importance}{
For each covariate, an overall variable importance measure is provided. The \code{m}-th forest computes the 
importance of the j-th covariate for the \code{m}-th class by recording the improvement in the splitting criterion 
at each split placed on such covariate. Summing over all such splits of the trees in the \code{m}-th 
forest gives the j-th covariate's importance for the \code{m}-th class. The overall variable importance measure of this 
covariate is then defined as the mean of its importances in each class.
}

\subsection{Honest Forests}{
Growing honest forests in a necessary requirements to conduct valid inference on the marginal effects. 
\code{morf} implements honest estimation as follows. The data set is split into a training sample and a honest
sample. Forests are grown using only the training sample, thus deriving the set of weights needed for prediction.
Then, the leaf estimates are replaced using the outcome from the honest sample (using the prediction method
outlined above). In this way, weights and outcomes are independent of each other, thereby allowing for valid
weight-based inference.
}
}
\references{
\itemize{
  \item Lechner, M., & Okasa, G. (2019). Random forest estimation of the ordered choice model. arXiv preprint arXiv:1907.02436. \doi{10.48550/arXiv.1907.02436}.
  \item Wright, M. N. & Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. J Stat Softw 77:1-17. \doi{10.18637/jss.v077.i01}.
}
}
\seealso{
\code{\link{predict.morf}}, \code{\link{marginal_effects}}
}
\author{
Riccardo Di Francesco
}
